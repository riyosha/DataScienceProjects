{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds an autocomplete model using n-grams. It has been adapted from an assignent from Coursera's NLP with Probababilistic Models Course (Week 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.data.path.append('.')\n",
    "import random\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'str'>\n",
      "Number of letters: 3335477\n",
      "First 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Last 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"ust had one a few weeks back....hopefully we will be back soon! wish you the best yo\\nColombia is with an 'o'...“: We now ship to 4 countries in South America (fist pump). Please welcome Columbia to the Stunner Family”\\n#GutsiestMovesYouCanMake Giving a cat a bath.\\nCoffee after 5 was a TERRIBLE idea.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n"
     ]
    }
   ],
   "source": [
    "# import twitter data - single files with line separated tweets\n",
    "with open(\"en_US.twitter.txt\", \"r\",encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "print(\"Data type:\", type(data))\n",
    "print(\"Number of letters:\", len(data))\n",
    "print(\"First 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[0:300])\n",
    "print(\"-------\")\n",
    "\n",
    "print(\"Last 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[-300:])\n",
    "print(\"-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#fns to split and tokenize  data into a list of lists of tokens\n",
    "def split_to_sentences(data):\n",
    "    sentences = data.split('\\n')\n",
    "    sentences = [x.strip() for x in sentences]\n",
    "    sentences = [x for x in sentences if len(x)>0]\n",
    "    return sentences\n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    sentences = [x.lower() for x in sentences]\n",
    "    tokens =[]\n",
    "    for text in sentences:\n",
    "        tokens.append(word_tokenize(text))\n",
    "    return tokens\n",
    "\n",
    "def get_tokenized_data(data):\n",
    "    tokenized_sentences=[]\n",
    "    sentences = split_to_sentences(data)\n",
    "    return tokenize_sentences(sentences)\n",
    "\n",
    "# fn to count frequency o tokens\n",
    "def count_words(tokenized_sentences):\n",
    "    word_counts = defaultdict(int)\n",
    "    for tokens in tokenized_sentences:\n",
    "        for token in tokens:\n",
    "            word_counts[token]+=1\n",
    "    return word_counts\n",
    "\n",
    "#fn to filter words which  occur at least n times\n",
    "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
    "\n",
    "    closed_vocab=[]\n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "\n",
    "    for word in word_counts.keys():\n",
    "        if word_counts[word]>=count_threshold:\n",
    "            closed_vocab.append(word)\n",
    "    \n",
    "    return closed_vocab\n",
    "\n",
    "# fn to replace infrequent words  with unknown token\n",
    "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "    replaced_tokenized_sentences=[]\n",
    "    for tokens in tokenized_sentences:\n",
    "        t=[]\n",
    "        for token in tokens:\n",
    "            if token not in vocabulary:\n",
    "                t.append(unknown_token)\n",
    "            else:\n",
    "                t.append(token)\n",
    "        replaced_tokenized_sentences.append(t)\n",
    "\n",
    "    return replaced_tokenized_sentences\n",
    "\n",
    "#fn to create train, test data and vocabulury using the above fns\n",
    "def preprocess_data(train_data, test_data, count_threshold): #train and test data are tokenized sentences\n",
    "    word_counts = count_words(train_data)\n",
    "    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)\n",
    "    preprocessed_train_data = replace_oov_words_by_unk(train_data, vocabulary)\n",
    "\n",
    "    preprocessed_test_data = replace_oov_words_by_unk(test_data, vocabulary)\n",
    "\n",
    "    return preprocessed_train_data, preprocessed_test_data, vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = get_tokenized_data(data)\n",
    "random.seed(87)\n",
    "random.shuffle(tokenized_data)\n",
    "\n",
    "train_size = int(len(tokenized_data) * 0.8)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bringing it all together  - preprocessing the data \n",
    "minimum_freq = 2\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n",
    "                                                                        test_data, \n",
    "                                                                        minimum_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the N-grams model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fn to make a dict of unique n_grams and their frequency  in the training data \n",
    "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
    "    n_grams = defaultdict(int)\n",
    "    updated_data=[]\n",
    "    for tokens in data:\n",
    "        new_token = tuple([start_token]*(n) + tokens + [end_token])\n",
    "        updated_data.append(new_token)\n",
    "\n",
    "    for tokens in updated_data:\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            n_grams[tuple(tokens[i:i+n])]+=1\n",
    "    \n",
    "    return n_grams\n",
    "\n",
    "# given a word and previous n-gram,fn to estimate its probability\n",
    "def estimate_probability(word, previous_n_gram, \n",
    "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    n_plus1_gram = previous_n_gram + (word,)\n",
    "    return (n_plus1_gram_counts[n_plus1_gram] + k)/(n_gram_counts[previous_n_gram]+k*vocabulary_size)\n",
    "\n",
    "# fn to estimate all probabilities  of  words possible after agiven n-gram\n",
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    "    word_probs=defaultdict(int)\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    for word in vocabulary:\n",
    "        word_probs[word]=estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, len(vocabulary),k)\n",
    "\n",
    "    return word_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fn to make a count matrix with n-grams as rows and vocabularyas columns\n",
    "def make_count_matrix(n_plus1_gram_counts, vocabulary,k=1):\n",
    "    vocabulary=vocabulary+[\"<e>\", \"<unk>\"]\n",
    "    n_grams = list(set([x[0:-1] for x in n_plus1_gram_counts.keys()]))\n",
    "\n",
    "    matrix = np.zeros((len(n_grams),len(vocabulary)))\n",
    "\n",
    "    for i in range(len(n_grams)):\n",
    "        for j in range(len(vocabulary)):\n",
    "            key = tuple(n_grams[i]) + (vocabulary[j],)\n",
    "            matrix[i,j] = n_plus1_gram_counts[key]\n",
    "    matrix = pd.DataFrame(matrix, index=n_grams, columns=vocabulary)\n",
    "    return matrix\n",
    "\n",
    "#fn to make a probability matrix fromthe count matrix                   \n",
    "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
    "    count_matrix = make_count_matrix(n_plus1_gram_counts,vocabulary,k)\n",
    "    count_matrix += k\n",
    "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-gram:\n",
      "defaultdict(<class 'int'>, {('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1})\n",
      "Bi-gram:\n",
      "defaultdict(<class 'int'>, {('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1})\n",
      "The estimated probability of word 'cat' given the previous n-gram 'a' is: 0.3333\n"
     ]
    }
   ],
   "source": [
    "# test code\n",
    "\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "print(\"Uni-gram:\")\n",
    "print(count_n_grams(sentences, 1))\n",
    "print(\"Bi-gram:\")\n",
    "print(count_n_grams(sentences, 2))\n",
    "\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "tmp_prob = estimate_probability(\"cat\", \"a\", unigram_counts, bigram_counts, len(unique_words), k=1)\n",
    "\n",
    "print(f\"The estimated probability of word 'cat' given the previous n-gram 'a' is: {tmp_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity - it is used to complare the performance of different ngram models. The lower the better\n",
    "from math import log\n",
    "from math import exp\n",
    "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "    perplexity=0\n",
    "    sentence = ['<s>']*n + sentence + ['<e>']\n",
    "    m = len(sentence)\n",
    "    for i in range(n,m):\n",
    "        perplexity+=log(estimate_probability(sentence[i], sentence[i-n:i], \n",
    "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0))\n",
    "    return exp((-1/m)*perplexity)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for first train sample: 2.8040\n",
      "Perplexity for test sample: 3.9654\n"
     ]
    }
   ],
   "source": [
    "#test the perplexity function\n",
    "perplexity_train1 = calculate_perplexity(sentences[0],\n",
    "                                         unigram_counts, bigram_counts,\n",
    "                                         len(unique_words), k=1.0)\n",
    "print(f\"Perplexity for first train sample: {perplexity_train1:.4f}\")\n",
    "\n",
    "test_sentence = ['i', 'like', 'a', 'dog']\n",
    "perplexity_test = calculate_perplexity(test_sentence,\n",
    "                                       unigram_counts, bigram_counts,\n",
    "                                       len(unique_words), k=1.0)\n",
    "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Autocomplete System "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn to suggest the most likely next word with its probability\n",
    "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    probabilities = estimate_probabilities(previous_n_gram,\n",
    "                                           n_gram_counts, n_plus1_gram_counts,\n",
    "                                           vocabulary, k=k)\n",
    "    if start_with!=None:\n",
    "        i = len(start_with)\n",
    "        probabilities = {k:v for k,v in probabilities.items() if k[:i]==start_with}\n",
    "    top_suggestion = sorted(probabilities.items(), key=lambda item: item[1], reverse=True)[0]\n",
    "    return (top_suggestion[0],top_suggestion[1])\n",
    "\n",
    "# fn to suggest multiple next words using different n_grams\n",
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "    model_counts = len(n_gram_counts_list)\n",
    "    suggestions = []\n",
    "    for i in range(model_counts-1):\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
    "        \n",
    "        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n",
    "                                    n_plus1_gram_counts, vocabulary,\n",
    "                                    k=k, start_with=start_with)\n",
    "        suggestions.append(suggestion)\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing n-gram counts with n = 1 ...\n",
      "Computing n-gram counts with n = 2 ...\n",
      "Computing n-gram counts with n = 3 ...\n",
      "Computing n-gram counts with n = 4 ...\n",
      "Computing n-gram counts with n = 5 ...\n"
     ]
    }
   ],
   "source": [
    "#we'll make multiple n gram models and get multiple suggestions from them\n",
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
    "    n_model_counts = count_n_grams(train_data_processed, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['hey', 'how', 'are', 'you'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"'re\", 0.0239720461563465),\n",
       " ('?', 0.002888086642599278),\n",
       " ('?', 0.001613228473482557),\n",
       " ('<e>', 0.00013489815189531904)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = input('Enter text for autocompletion: ')\n",
    "previous_tokens = get_tokenized_data(sample)[0]\n",
    "start = input('Optional: word starts with: ')\n",
    "\n",
    "if start!=\"\":\n",
    "    suggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0,start_with=start)\n",
    "else:\n",
    "    suggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(suggestion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
